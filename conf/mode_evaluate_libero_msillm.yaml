defaults:
  - datamodule: libero
  - model: mode_agent

eval_cfg_overwrite:
  model:
    num_sampling_steps: 10

train_folder: /home/hjkim/MoDE_Diffusion_Policy/saved_models
checkpoint: msillm-NeuralCompression_main-msillm_quality_vlo1_epoch24.ckpt
device: 0

log_dir: /home/hjkim/MoDE_Diffusion_Policy/outputs/libero_eval
dataset_path: /tmp
num_videos: 1
debug: False

log_wandb: True
wandb_entity: ${oc.env:WANDB_ENTITY,null}

# MuJoCo / EGL rendering config
# If CUDA illegal memory access persists and you see EGL errors, set `mujoco_egl_device_id`
# to the physical GPU index (as shown in `nvidia-smi -L`), and/or set `mujoco_gl: osmesa`
# to force CPU rendering for debugging.
mujoco_gl: egl
mujoco_egl_device_id: null
egl_device_id: null

# CUDA debug knobs (do NOT change model architecture; only affects execution/scheduling)
cuda_debug:
  deterministic: false   # uses deterministic algorithms when possible
  disable_tf32: false    # disable TF32 matmul (changes kernel choice)
  sync_each_step: false  # add torch.cuda.synchronize() after model.step/env.step to localize async errors

num_sampling_steps: null
sampler_type: null
multistep: null
sigma_min: null
sigma_max: null
noise_scheduler: null

num_sequences: 50
max_steps: 520
n_eval: 50
task_embedding_format: clip

benchmark_name: libero_10 # [LIBERO_SPATIAL, LIBERO_OBJECT, LIBERO_GOAL, LIBERO_90, LIBERO_10]

hydra:
  run:
    dir: outputs/${checkpoint}  # Directory name includes .ckpt, but we'll use checkpoint stem in code
  job:
    name: ${checkpoint}  # Job name includes .ckpt extension

