defaults:
  - callbacks: libero
  - datamodule: libero
  - model: mode_agent
  - override hydra/job_logging: colorlog
  - override hydra/hydra_logging: colorlog
  - _self_

# MoDE_Policy (LIBERO) data root.
# NOTE: the LIBERO datamodule / dataset defaults to `libero.get_libero_path("datasets")`.
# Use this as a stable project-level knob and/or to drive `custom_data_path` when needed.
root_data_dir: ${oc.env:MODE_ROOT_DATA_DIR,${oc.env:LIBERO_DATA_DIR,./data}}
#
# Ensure the bundled LIBERO repo is importable (fixes `ModuleNotFoundError: libero`)
# without requiring a global PYTHONPATH export.
libero_repo_dir: ${hydra:runtime.cwd}/LIBERO
lang_folder: lang_clip_resnet50
vis_clip_model_name: ViT-B/16
clip_lang_model_name: ViT-B/32

log_dir: ./logs
slurm: false
future_range: 29
seed: 42
device: 'cuda'
batch_size: 128 # 38 # 128
# Number of GPU devices to use (passed through to `trainer.devices` below).
devices: 2
goal_window_size: 1
act_dim: 7
proprio_dims: 9
obs_dim: 512
goal_dim: 512
obs_seq_len: 1
act_seq_len: 10
multistep: ${act_seq_len}
p_last_state: 0
gen_img_res: 112
max_epochs: 20
rollout_lh_skip_epochs: 9
num_workers: 1
benchmark_name: ${libero_benchmark} 
libero_benchmark: libero_10

msillm:
  # torch.hub entrypoint (NeuralCompression hubconf) for MS-ILLM
  hub_repo: facebookresearch/NeuralCompression:v0.3.1
  entrypoint: msillm_quality_vlo1
  pretrained: true

# Override checkpoint callback for MS-ILLM training
# Since rollout_lh callback is disabled, eval_lh/avg_seq_len is not available
# Use val_act/lang_act_loss_pp instead for monitoring
callbacks:
  checkpoint:
    save_top_k: 3
    monitor: val_act/lang_act_loss_pp  # Changed from eval_lh/avg_seq_len since rollout_lh callback is disabled
    mode: min  # Changed from max since we want to minimize loss
    dirpath: ${hydra:runtime.cwd}/saved_models  # Use absolute path for clarity
    filename: 'epoch={epoch:02d}'  # Simplified filename - PyTorch Lightning creates directories when metric name contains '/'

# Pretrained MoDE weights from Hugging Face
# Set to Hugging Face repo ID (e.g., "mbreuss/MoDE_LIBERO_10") or local file path
pretrain_chk: "mbreuss/MoDE_LIBERO_10"
pretrain_chk_filename: "model_cleaned.safetensors"  # optional, default: model_cleaned.safetensors


trainer:
  devices: ${devices}
  precision: bf16
  max_epochs: ${max_epochs}
  sync_batchnorm: False
  accelerator: auto
  limit_train_batches: 1000
  limit_val_batches: 4
  # gradient_clip_val: 1

logger:
  _target_: pytorch_lightning.loggers.WandbLogger
  save_dir: .
  name: logger
  group: mode
  log_model: false
  project: ${libero_benchmark}
  # Prefer environment variables; training script also overwrites group/name/id for uniqueness.
  entity: ${oc.env:WANDB_ENTITY,null}
  id: ${oc.env:WANDB_RUN_ID,null}


hydra:
  run:
    dir: ${log_dir}/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
  sweep:
    dir: ${log_dir}/runs/${now:%Y-%m-%d}/${now:%H-%M-%S}
    subdir: ${hydra.job.override_dirname}
  job:
    env_set:
      # Avoid interpolation issues during Hydra initialization; compute directly.
      PYTHONPATH: ${hydra:runtime.cwd}/LIBERO:${oc.env:PYTHONPATH,}
    config:
      override_dirname:
        exclude_keys:
          - log_dir
          - datamodule.root_data_dir
          - trainer.devices
          - num_workers
          - trainer.limit_train_batches
          - trainer.limit_val_batches
